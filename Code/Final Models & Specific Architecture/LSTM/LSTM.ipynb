{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9e22b7",
   "metadata": {},
   "source": [
    "Uji ke 10 Data awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Paths & params\n",
    "TRAIN_CSV   = '/kaggle/input/data-btc/fix3/fix2/train.csv'\n",
    "VAL_CSV     = '/kaggle/input/data-btc/fix3/fix2/val.csv'\n",
    "TEST_CSV    = '/kaggle/input/data-btc/fix3/fix2/test.csv'\n",
    "WINDOW_SIZE = 48\n",
    "BATCH_SIZE  = 64\n",
    "TUNE_EPOCHS = 5\n",
    "FINAL_EPOCHS= 5\n",
    "LR          = 1e-3\n",
    "PATIENCE    = 3\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED        = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 1) Load raw\n",
    "df_tr_raw   = pd.read_csv(TRAIN_CSV, parse_dates=['timestamp'])\n",
    "df_va_raw   = pd.read_csv(VAL_CSV,   parse_dates=['timestamp'])\n",
    "df_te_raw   = pd.read_csv(TEST_CSV,  parse_dates=['timestamp'])\n",
    "\n",
    "# 2) Define horizons\n",
    "HORIZONS = ['1h','2h','3h','6h','12h','1d','3d','7d','15d','30d']\n",
    "LABELS   = [f'label_{h}' for h in HORIZONS]\n",
    "FEATS    = [c for c in df_tr_raw.columns if c not in LABELS+['timestamp','close_time']]\n",
    "\n",
    "# 3) Scale train & val\n",
    "scaler_X = MinMaxScaler().fit(df_tr_raw[FEATS])\n",
    "scaler_y = MinMaxScaler().fit(df_tr_raw[LABELS])\n",
    "\n",
    "df_tr = df_tr_raw.copy()\n",
    "df_va = df_va_raw.copy()\n",
    "df_tr[FEATS] = scaler_X.transform(df_tr[FEATS])\n",
    "df_va[FEATS] = scaler_X.transform(df_va[FEATS])\n",
    "df_tr[LABELS] = scaler_y.transform(df_tr[LABELS])\n",
    "df_va[LABELS] = scaler_y.transform(df_va[LABELS])\n",
    "\n",
    "# 4) make_sequences\n",
    "def make_sequences(df_scaled, raw_close, raw_label, window):\n",
    "    X, Y, LP, Y_true = [], [], [], []\n",
    "    arr_f = df_scaled[FEATS].values\n",
    "    N     = len(df_scaled)\n",
    "    for i in range(N - window):\n",
    "        seq        = arr_f[i:i+window]\n",
    "        last_price = raw_close[i+window-1]\n",
    "        future     = raw_label[i+window]\n",
    "        delta      = (future - last_price) / (last_price + 1e-8)\n",
    "        X.append(seq); Y.append(delta)\n",
    "        LP.append(last_price); Y_true.append(future)\n",
    "    if not X:\n",
    "        return None,None,None,None\n",
    "    return np.stack(X), np.stack(Y), np.array(LP), np.stack(Y_true)\n",
    "\n",
    "# 5) Build train+val sequences\n",
    "raw_close_tv = pd.concat([df_tr_raw, df_va_raw], ignore_index=True)['close'].values\n",
    "raw_label_tv = pd.concat([df_tr_raw[LABELS], df_va_raw[LABELS]], ignore_index=True).values\n",
    "df_tv_scaled = pd.concat([df_tr, df_va], ignore_index=True)\n",
    "\n",
    "X_all, Y_all, LP_all, TRUE_all = make_sequences(\n",
    "    df_tv_scaled, raw_close_tv, raw_label_tv, WINDOW_SIZE\n",
    ")\n",
    "n_tr_seq = len(df_tr_raw) - WINDOW_SIZE\n",
    "X_tr, Y_tr = X_all[:n_tr_seq], Y_all[:n_tr_seq]\n",
    "LP_tr, TRUE_tr = LP_all[:n_tr_seq], TRUE_all[:n_tr_seq]\n",
    "X_va, Y_va = X_all[n_tr_seq:], Y_all[n_tr_seq:]\n",
    "LP_va, TRUE_va = LP_all[n_tr_seq:], TRUE_all[n_tr_seq:]\n",
    "\n",
    "# 6) Build TEST sequences (prepend last WINDOW_SIZE of val)\n",
    "raw_close_te = np.concatenate([\n",
    "    df_va_raw['close'].values[-WINDOW_SIZE:],\n",
    "    df_te_raw['close'].values\n",
    "])\n",
    "raw_label_te = np.concatenate([\n",
    "    df_va_raw[LABELS].values[-WINDOW_SIZE:],\n",
    "    df_te_raw[LABELS].values\n",
    "])\n",
    "df_va_scaled       = df_va.copy()\n",
    "df_va_scaled[FEATS]= scaler_X.transform(df_va_scaled[FEATS])\n",
    "df_te_scaled       = df_te_raw.copy()\n",
    "df_te_scaled[FEATS]= scaler_X.transform(df_te_scaled[FEATS])\n",
    "df_comb_scaled     = pd.concat([\n",
    "    df_va_scaled.iloc[-WINDOW_SIZE:],\n",
    "    df_te_scaled[FEATS]\n",
    "], ignore_index=True)\n",
    "\n",
    "X_te, Y_te, LP_te, TRUE_te = make_sequences(\n",
    "    df_comb_scaled, raw_close_te, raw_label_te, WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# 7) DataLoaders\n",
    "train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr))\n",
    "val_ds   = TensorDataset(torch.FloatTensor(X_va), torch.FloatTensor(Y_va))\n",
    "test_ds  = None if X_te is None else TensorDataset(torch.FloatTensor(X_te), torch.FloatTensor(Y_te))\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_ld  = None if test_ds is None else DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 8) Model (unchanged)\n",
    "class MultiHorizonModel(nn.Module):\n",
    "    def __init__(self, n_feat, n_hor, d_emb=8):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(n_feat, 64, kernel_size=3, padding=2, dilation=2)\n",
    "        self.lstm = nn.LSTM(64, 128, batch_first=True, bidirectional=True)\n",
    "        self.norm = nn.LayerNorm(256)\n",
    "        self.attn = nn.MultiheadAttention(256, num_heads=4, batch_first=True)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.h_emb= nn.Embedding(n_hor, d_emb)\n",
    "        self.heads= nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(256+d_emb,64), nn.ReLU(),\n",
    "                          nn.Dropout(0.2), nn.Linear(64,1))\n",
    "            for _ in range(n_hor)\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "        b = x.size(0)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x,_ = self.lstm(x)\n",
    "        x = self.norm(x)\n",
    "        att,_ = self.attn(x,x,x)\n",
    "        x = self.norm(x+att)\n",
    "        x = x.permute(0,2,1)\n",
    "        feat = self.pool(x).squeeze(-1)\n",
    "        outs=[]\n",
    "        for i, head in enumerate(self.heads):\n",
    "            emb = self.h_emb(torch.full((b,), i, dtype=torch.long, device=feat.device))\n",
    "            outs.append(head(torch.cat([feat,emb],1)))\n",
    "        return torch.cat(outs,1)\n",
    "\n",
    "# eval helper\n",
    "def eval_phase(model, LP, TRUE, desc):\n",
    "    print(f\"--- {desc} LP first10:\", LP[:10])\n",
    "    for hi,h in enumerate(HORIZONS):\n",
    "        print(f\"--- {desc} TRUE_{h} first10:\", TRUE[:10,hi])\n",
    "    preds=[]; trues=TRUE[:10]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "            xb = torch.FloatTensor(X_te[i:i+1]).to(DEVICE)\n",
    "            out= model(xb).cpu().numpy()[0]\n",
    "            preds.append(out)\n",
    "    preds = np.stack(preds)  # (10,10)\n",
    "    price_pred = np.zeros_like(preds)\n",
    "    for i in range(10):\n",
    "        price_pred[i] = LP[i] * (1+preds[i])\n",
    "    # plot\n",
    "    for hi,h in enumerate(HORIZONS):\n",
    "        rmse = mean_squared_error(trues[:,hi], price_pred[:,hi], squared=False)\n",
    "        mape = mean_absolute_percentage_error(trues[:,hi], price_pred[:,hi])*100\n",
    "        print(f\"{desc} {h}: RMSE={rmse:.2f}, MAPE={mape:.2f}%\")\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(trues[:,hi], label='Actual')\n",
    "        plt.plot(price_pred[:,hi],'--', label='Predicted')\n",
    "        plt.xticks(range(10))\n",
    "        plt.title(f\"{desc} - {h}\")\n",
    "        plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 9) Phase 1: TRAIN→VAL (no tune)\n",
    "print(\"=== PHASE 1: TRAIN→VAL ===\")\n",
    "model = MultiHorizonModel(n_feat=X_tr.shape[2], n_hor=len(HORIZONS)).to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "crit  = nn.HuberLoss(delta=1.0)\n",
    "weights = torch.linspace(1,2,steps=len(HORIZONS)).to(DEVICE)\n",
    "\n",
    "best_val, cnt = float('inf'), 0\n",
    "for ep in range(1, TUNE_EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,yb in train_ld:\n",
    "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        loss  = (crit(model(xb), yb)*weights).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    # validate\n",
    "    val_losses=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in val_ld:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            val_losses.append(((crit(model(xb), yb)*weights).mean().item()))\n",
    "    mv = np.mean(val_losses)\n",
    "    print(f\"Ep{ep} val_loss={mv:.4f}\")\n",
    "    if mv<best_val:\n",
    "        best_val, cnt = mv, 0\n",
    "        torch.save(model.state_dict(), 'best_tune.pth')\n",
    "    else:\n",
    "        cnt+=1\n",
    "        if cnt>=PATIENCE:\n",
    "            print(\"Early stopping\"); break\n",
    "\n",
    "# eval on VAL sequences\n",
    "eval_phase(model, LP_va, TRUE_va, desc=\"Phase1 VAL\")\n",
    "\n",
    "# 10) Phase 2: TRAIN+VAL→TEST (final retrain & eval)\n",
    "print(\"\\n=== PHASE 2: TRAIN+VAL→TEST ===\")\n",
    "model.load_state_dict(torch.load('best_tune.pth'))\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# combine train+val\n",
    "X_comb = np.vstack([X_tr, X_va]); Y_comb = np.vstack([Y_tr, Y_va])\n",
    "comb_ds  = TensorDataset(torch.FloatTensor(X_comb), torch.FloatTensor(Y_comb))\n",
    "comb_ld  = DataLoader(comb_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "for ep in range(1, FINAL_EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,yb in comb_ld:\n",
    "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        loss  = (crit(model(xb), yb)*weights).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "# eval on TEST sequences\n",
    "eval_phase(model, LP_te, TRUE_te, desc=\"Phase2 TEST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1489f7",
   "metadata": {},
   "source": [
    "Uji ke 100 data awal dan simpan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Paths & params\n",
    "TRAIN_CSV    = '/kaggle/input/data-btc/fix3/fix2/train.csv'\n",
    "VAL_CSV      = '/kaggle/input/data-btc/fix3/fix2/val.csv'\n",
    "TEST_CSV     = '/kaggle/input/data-btc/fix3/fix2/test.csv'\n",
    "WINDOW_SIZE  = 48\n",
    "BATCH_SIZE   = 64\n",
    "TUNE_EPOCHS  = 10\n",
    "FINAL_EPOCHS = 10\n",
    "LR           = 1e-3\n",
    "PATIENCE     = 3\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED         = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 1) Load raw\n",
    "df_tr_raw = pd.read_csv(TRAIN_CSV, parse_dates=['timestamp'])\n",
    "df_va_raw = pd.read_csv(VAL_CSV,   parse_dates=['timestamp'])\n",
    "df_te_raw = pd.read_csv(TEST_CSV,  parse_dates=['timestamp'])\n",
    "\n",
    "# 2) Horizons & features\n",
    "HORIZONS = ['1h','2h','3h','6h','12h','1d','3d','7d','15d','30d']\n",
    "LABELS   = [f'label_{h}' for h in HORIZONS]\n",
    "FEATS    = [c for c in df_tr_raw.columns if c not in LABELS+['timestamp','close_time']]\n",
    "\n",
    "# 3) Scale train & val\n",
    "scaler_X = MinMaxScaler().fit(df_tr_raw[FEATS])\n",
    "scaler_y = MinMaxScaler().fit(df_tr_raw[LABELS])\n",
    "\n",
    "df_tr = df_tr_raw.copy()\n",
    "df_va = df_va_raw.copy()\n",
    "df_tr[FEATS]  = scaler_X.transform(df_tr[FEATS])\n",
    "df_va[FEATS]  = scaler_X.transform(df_va[FEATS])\n",
    "df_tr[LABELS] = scaler_y.transform(df_tr[LABELS])\n",
    "df_va[LABELS] = scaler_y.transform(df_va[LABELS])\n",
    "\n",
    "# 4) Sequence builder\n",
    "def make_sequences(df_scaled, raw_close, raw_label, window):\n",
    "    X, Y, LP, Y_true = [], [], [], []\n",
    "    arr_f = df_scaled[FEATS].values\n",
    "    N     = len(df_scaled)\n",
    "    for i in range(N - window):\n",
    "        seq        = arr_f[i:i+window]\n",
    "        last_price = raw_close[i+window-1]\n",
    "        future     = raw_label[i+window]\n",
    "        delta      = (future - last_price) / (last_price + 1e-8)\n",
    "        X.append(seq); Y.append(delta)\n",
    "        LP.append(last_price); Y_true.append(future)\n",
    "    if not X:\n",
    "        return None,None,None,None\n",
    "    return np.stack(X), np.stack(Y), np.array(LP), np.stack(Y_true)\n",
    "\n",
    "# 5) Build train+val sequences\n",
    "raw_close_tv = pd.concat([df_tr_raw, df_va_raw], ignore_index=True)['close'].values\n",
    "raw_label_tv = pd.concat([df_tr_raw[LABELS], df_va_raw[LABELS]], ignore_index=True).values\n",
    "df_tv_scaled = pd.concat([df_tr, df_va], ignore_index=True)\n",
    "\n",
    "X_all, Y_all, LP_all, TRUE_all = make_sequences(\n",
    "    df_tv_scaled, raw_close_tv, raw_label_tv, WINDOW_SIZE\n",
    ")\n",
    "n_tr_seq = len(df_tr_raw) - WINDOW_SIZE\n",
    "X_tr, Y_tr       = X_all[:n_tr_seq], Y_all[:n_tr_seq]\n",
    "LP_tr, TRUE_tr   = LP_all[:n_tr_seq], TRUE_all[:n_tr_seq]\n",
    "X_va, Y_va       = X_all[n_tr_seq:], Y_all[n_tr_seq:]\n",
    "LP_va, TRUE_va   = LP_all[n_tr_seq:], TRUE_all[n_tr_seq:]\n",
    "\n",
    "# 6) Build TEST sequences (prepend last WINDOW_SIZE of val)\n",
    "raw_close_te = np.concatenate([df_va_raw['close'].values[-WINDOW_SIZE:], df_te_raw['close'].values])\n",
    "raw_label_te = np.concatenate([df_va_raw[LABELS].values[-WINDOW_SIZE:], df_te_raw[LABELS].values])\n",
    "\n",
    "df_va_scaled        = df_va.copy();   df_va_scaled[FEATS] = scaler_X.transform(df_va[FEATS])\n",
    "df_te_scaled        = df_te_raw.copy(); df_te_scaled[FEATS] = scaler_X.transform(df_te_raw[FEATS])\n",
    "df_comb_scaled      = pd.concat([df_va_scaled.iloc[-WINDOW_SIZE:], df_te_scaled[FEATS]], ignore_index=True)\n",
    "\n",
    "X_te, Y_te, LP_te, TRUE_te = make_sequences(df_comb_scaled, raw_close_te, raw_label_te, WINDOW_SIZE)\n",
    "\n",
    "# 7) DataLoaders\n",
    "train_ld = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr)),\n",
    "                      batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_ld   = DataLoader(TensorDataset(torch.FloatTensor(X_va), torch.FloatTensor(Y_va)),\n",
    "                      batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_ld  = None if X_te is None else DataLoader(TensorDataset(torch.FloatTensor(X_te), torch.FloatTensor(Y_te)),\n",
    "                                                batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 8) Model\n",
    "class MultiHorizonModel(nn.Module):\n",
    "    def __init__(self, n_feat, n_hor, d_emb=8):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(n_feat,64,3,padding=2,dilation=2)\n",
    "        self.lstm = nn.LSTM(64,128,batch_first=True,bidirectional=True)\n",
    "        self.norm = nn.LayerNorm(256)\n",
    "        self.attn = nn.MultiheadAttention(256, num_heads=4, batch_first=True)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.h_emb= nn.Embedding(n_hor, d_emb)\n",
    "        self.heads= nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(256+d_emb,64), nn.ReLU(),\n",
    "                          nn.Dropout(0.2), nn.Linear(64,1))\n",
    "            for _ in range(n_hor)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x,_ = self.lstm(x)\n",
    "        x = self.norm(x)\n",
    "        att,_ = self.attn(x,x,x)\n",
    "        x = self.norm(x+att)\n",
    "        x = x.permute(0,2,1)\n",
    "        feat = self.pool(x).squeeze(-1)\n",
    "        outs = []\n",
    "        for i, head in enumerate(self.heads):\n",
    "            emb = self.h_emb(torch.full((b,), i, dtype=torch.long, device=feat.device))\n",
    "            outs.append(head(torch.cat([feat, emb],1)))\n",
    "        return torch.cat(outs,1)\n",
    "\n",
    "# 9) Eval helper\n",
    "def eval_phase(model, LP, TRUE, desc, first_n=100):\n",
    "    n_pts = min(first_n, LP.shape[0])\n",
    "    print(f\"--- {desc} LP first {n_pts}: {LP[:n_pts]}\")\n",
    "    for hi,h in enumerate(HORIZONS):\n",
    "        print(f\"--- {desc} TRUE_{h} first {n_pts}: {TRUE[:n_pts,hi]}\")\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_pts):\n",
    "            xb = torch.FloatTensor(X_te[i:i+1]).to(DEVICE)\n",
    "            out= model(xb).cpu().numpy()[0]\n",
    "            preds.append(out)\n",
    "    preds = np.stack(preds)\n",
    "    price_pred = np.zeros_like(preds)\n",
    "    for i in range(n_pts):\n",
    "        price_pred[i] = LP[i] * (1 + preds[i])\n",
    "    price_true = TRUE[:n_pts]\n",
    "    for hi,h in enumerate(HORIZONS):\n",
    "        rmse = mean_squared_error(price_true[:,hi], price_pred[:,hi], squared=False)\n",
    "        mape = mean_absolute_percentage_error(price_true[:,hi], price_pred[:,hi])*100\n",
    "        print(f\"{desc} {h}: RMSE={rmse:.2f}, MAPE={mape:.2f}%\")\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(price_true[:,hi], label='Actual')\n",
    "        plt.plot(price_pred[:,hi],'--', label='Predicted')\n",
    "        plt.xticks(range(0, n_pts, max(1, n_pts//10)))\n",
    "        plt.title(f\"{desc} - {h}\")\n",
    "        plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 10) Phase 1: TRAIN→VAL (no tuning)\n",
    "print(\"=== PHASE 1: TRAIN→VAL ===\")\n",
    "model = MultiHorizonModel(n_feat=X_tr.shape[2], n_hor=len(HORIZONS)).to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "crit  = nn.HuberLoss(delta=1.0)\n",
    "weights = torch.linspace(1,2,steps=len(HORIZONS)).to(DEVICE)\n",
    "\n",
    "best_val, cnt = float('inf'), 0\n",
    "for ep in range(1, TUNE_EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,yb in train_ld:\n",
    "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        loss  = (crit(model(xb), yb)*weights).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    # validate\n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in val_ld:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            val_losses.append(((crit(model(xb), yb)*weights).mean().item()))\n",
    "    mv = np.mean(val_losses)\n",
    "    print(f\"Ep{ep} val_loss={mv:.4f}\")\n",
    "    if mv < best_val:\n",
    "        best_val, cnt = mv, 0\n",
    "        torch.save(model.state_dict(), 'best_tune.pth')\n",
    "    else:\n",
    "        cnt += 1\n",
    "        if cnt >= PATIENCE:\n",
    "            print(\"Early stopping\"); break\n",
    "\n",
    "eval_phase(model, LP_va, TRUE_va, desc=\"Phase1 VAL\", first_n=100)\n",
    "\n",
    "# 11) Phase 2: TRAIN+VAL→TEST (final retrain, save & eval)\n",
    "print(\"\\n=== PHASE 2: TRAIN+VAL→TEST ===\")\n",
    "model.load_state_dict(torch.load('best_tune.pth'))\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "X_comb = np.vstack([X_tr, X_va])\n",
    "Y_comb = np.vstack([Y_tr, Y_va])\n",
    "comb_ds = TensorDataset(torch.FloatTensor(X_comb), torch.FloatTensor(Y_comb))\n",
    "comb_ld = DataLoader(comb_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for ep in range(1, FINAL_EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb,yb in comb_ld:\n",
    "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        loss  = (crit(model(xb), yb)*weights).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "# save final model once\n",
    "save_path = '/kaggle/working/lstm_final.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Saved final LSTM model to {save_path}\")\n",
    "\n",
    "# evaluate\n",
    "if test_ld is not None:\n",
    "    eval_phase(model, LP_te, TRUE_te, desc=\"Phase2 TEST\", first_n=100)\n",
    "else:\n",
    "    print(\"Test split too small, skipping.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
